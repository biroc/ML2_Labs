{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lab 3: EM and VAE\n",
    "\n",
    "### Machine Learning 2 (2016/2017)\n",
    "\n",
    "* The lab exercises should be made in groups of two people or individually.\n",
    "* The hand-in deadline is Wednesday, May 24, 23:59.\n",
    "* Assignment should be sent to p.j.j.p.versteeg@uva.nl. The subject line of your email should be \"[ML2_2017] lab#_lastname1\\_lastname2\". \n",
    "* Put your and your teammates' names in the body of the email\n",
    "* Attach the .IPYNB (IPython Notebook) file containing your code and answers. Naming of the file follows the same rule as the subject line. For example, if the subject line is \"[ML2_2017] lab02\\_Bongers\\_Blom\", the attached file should be \"lab02\\_Bongers\\_Blom.ipynb\". Only use underscores (\"\\_\") to connect names, otherwise the files cannot be parsed.\n",
    "\n",
    "Notes on implementation:\n",
    "\n",
    "* You should write your code and answers in an IPython Notebook: http://ipython.org/notebook.html. If you have problems, please ask or e-mail Philip.\n",
    "* Among the first lines of your notebook should be \"%pylab inline\". This imports all required modules, and your plots will appear inline.\n",
    "* NOTE: test your code and make sure we can run your notebook / scripts before you send them!\n",
    "\n",
    "$\\newcommand{\\bx}{\\mathbf{x}} \\newcommand{\\bpi}{\\mathbf{\\pi}} \\newcommand{\\bmu}{\\mathbf{\\mu}} \\newcommand{\\bX}{\\mathbf{X}} \\newcommand{\\bZ}{\\mathbf{Z}} \\newcommand{\\bz}{\\mathbf{z}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### MNIST data\n",
    "\n",
    "In this Lab we will use several methods for unsupervised learning on the MNIST dataset of written digits. The dataset contains digital images of handwritten numbers $0$ through $9$. Each image has 28x28 pixels that each take 256 values in a range from white ($= 0$) to  black ($=1$). The labels belonging to the images are also included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import cPickle, gzip\n",
    "\n",
    "# load data\n",
    "with gzip.open('mnist_small.gz', 'r') as f:\n",
    "    mnist_images, mnist_labels = cPickle.load(f)\n",
    "\n",
    "mnist_dim = (28, 28)\n",
    "print \"MNIST data loaded;\", \\\n",
    "    \" number of samples:\", len(mnist_labels), \\\n",
    "    '; number of pixels per sample:', mnist_dim[0], 'x', mnist_dim[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 1: Expectation Maximization\n",
    "We use the Expectation Maximization (EM) algorithm for the recognition of handwritten digits in the MNIST dataset. The images are modelled as a Bernoulli mixture model (see Bishop $\\S9.3.3$):\n",
    "$$\n",
    "p(\\bx|\\bmu, \\bpi) = \\sum_{k=1}^K  \\pi_k \\prod_{i=1}^D \\mu_{ki}^{x_i}(1-\\mu_{ki})^{(1-x_i)}\n",
    "$$\n",
    "where $x_i$ is the value of pixel $i$ in an image, $\\mu_{ki}$ represents the probability that pixel $i$ in class $k$ is black, and $\\{\\pi_1, \\ldots, \\pi_K\\}$ are the mixing coefficients of classes in the data. We want to use this data set to classify new images of handwritten numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.1 Binary data (5)\n",
    "As we like to apply our Bernoulli mixture model, convert the MNIST data to binary images, where each pixel $x_i \\in \\{0,1\\}$, by thresholding the dataset at an appropriate level. Sample a few images of digits $2$, $3$ and $4$; and show both the original and the binarized image together with their label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.2 Implementation (40)\n",
    "Write a function ```EM(batch, K, max_iter)``` that implements the EM algorithm on the Bernoulli mixture model. \n",
    "\n",
    "The only parameters your function should have are:\n",
    "* ```batch``` :: input training images\n",
    "* ```K``` :: size of the latent space\n",
    "* ```max_iter``` :: maximum number of iterations, i.e. one E-step and one M-step\n",
    "\n",
    "You are free to specify your return statement.\n",
    "\n",
    "Make sure you use a sensible way of terminating the iteration process early to prevent unnecessarily running through all epochs. Vectorize equations in ```numpy``` as  much as possible and clearly comment in your code where the E-step and where the M-step are performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def EM(batch, K, max_iter):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.2 Three digits test (10)\n",
    "In analogue with Bishop $\\S9.3.3$, sample a training set consisting of only __binary__ images of written digits $2$, $3$, and $4$. Run your EM algorithm and show the reconstructed digits. What are the identified mixing coefficients, and how do these compare to the true ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.3 More experiments (20)\n",
    "Perform the follow-up experiments listed below using your implementation of the EM algorithm. For each of these, describe/comment on the obtained results and give an explanation.\n",
    "\n",
    "* __1.3.1__\n",
    "    Use with more or less classes for $K$.\n",
    "* __1.3.2__ \n",
    "    Use the image labels in ```mnist_labels``` to identify some misclassified images and see if you understand why.\n",
    "* __1.3.3__ \n",
    "    Initialize the three classes with the true values of the parameters and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.4 Relation to Variational Inference (5)\n",
    "Propose how you would alter the Bernoulli mixture model for use with a variational method (Bishop Ch.$10$). \n",
    "\n",
    "Which priors would you use and why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "_YOUR ANSWER HERE_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 2: Variational Auto-Encoder\n",
    "A Variational Auto-Encoder (VAE) is a probabilistic model $p(\\bx, \\bz)$ over observed variables $\\bx$ and latent variables and/or parameters $\\bz$. Here we distinguish the decoder part, $p(\\bx | \\bz) p(\\bz)$ and an encoder part, that are both specified with a neural network. A lower bound on the log marginal likelihood $\\log p(\\bx)$ can be obtained by approximately inferring the latent variables z from the observed data x using an encoder distribution $q(\\bz| \\bx)$ that is also specified as a neural network. This lower bound is then optimized to fit the model to the data. \n",
    "\n",
    "More information can be found here:\n",
    "* Original paper by Kingma (2014) https://arxiv.org/pdf/1312.6114.pdf \n",
    "* Nice but large summary (2016) https://arxiv.org/pdf/1606.05908.pdf\n",
    "* One of _many_ online Tutorials, i.e. https://jaan.io/what-is-variational-autoencoder-vae-tutorial or http://kvfrans.com/variational-autoencoders-explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.1 The q-distribution (5 points)\n",
    "\n",
    "In variational inference, we introduce distribution $q(\\theta)$ over parameters / latent variables in order to make inference tractable. We can think of $q$ as being an approximation of a certain distribution. What function does $q$ approximate, $p(\\bx|\\theta)$, $p(\\theta|\\bx)$, $p(\\bx, \\theta)$, $p(\\theta)$, or $p(\\bx)$, and how do you see that from the equation $\\ln p(\\bx) = \\mathcal{L}(q) + \\mathrm{KL}(q||p)$? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Answer**: **q** approximates the posterior distribution of the latent variables (in this case represented as $\\theta$) conditioned on the observed variables (x), i.e. $P(\\theta|x)$. To explain the last formula, let's start by considering what we are trying to achieve. Given a posterior distribution that is intractable due to working in a high dimensional regime, or not having a computable analytic form, we posit a family of distributions $q$ with the intention of constructing an approximate distribution as close as possible to the posterior (where 'closeness' is measured by KL divergence). We therefore want $q^{*} = argmin_{q} KL(q(\\theta)|p(\\theta|x))$:\n",
    "\n",
    "$$KL(q(\\theta)|p(\\theta|x)) = E_{q(\\theta)}[log q(\\theta) - log p(\\theta|x)]$$\n",
    "\n",
    "We apply Bayes theorem to the second component of the expectation:\n",
    "\n",
    "$$KL(q(\\theta)|p(\\theta|x)) = E_{q(\\theta)}[log q(\\theta) - log(p(\\theta,x) + log p(x)]$$\n",
    "\n",
    "Notice further that the log of the marginal likelihood $log p(x)$ does not depend on $\\theta$:\n",
    "\n",
    "$$KL(q(\\theta)|p(\\theta|x)) = E_{q(\\theta)}[log q(\\theta) - log(p(\\theta,x)] + log p(x)$$\n",
    "\n",
    "If we multiply the expression by -1 on both sides and isolate the log of the marginal likelihood we obtain:\n",
    "\n",
    "$$log p(x) = E_{q(\\theta)}[log(p(\\theta,x) - log q(\\theta)] + KL(q||p)$$\n",
    "$$log p(x) = L(q) + KL(q||p)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.2 Implementation (20 points)\n",
    "Above is a version of a Variational Auto-Encoder that uses PyTorch (http://pytorch.org/). PyTorch is python package that is  particularly suited for high level programming of neural net, and is easily installable by following the instructions on their website.\n",
    "\n",
    "Add comments to the code below, where each ```COMMENT``` line should be replaced with an explanation of the code on the line below it (this may also be one or two words) and specify the encoder and decoder parts. \n",
    "\n",
    "Modify the ```run(...)``` function to visualize each $200$ iterations how the model is improving. Do this by sampling a few values of the latent space $\\bz$ and running those through the decoder. Add a plot of the loss function and show this after convergence. Run the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "## COMMENT\n",
    "N_samples, X_dim = mnist_images.shape \n",
    "## COMMENT\n",
    "mb_size = 64\n",
    "## COMMENT\n",
    "Z_dim = 100 \n",
    "## COMMENT\n",
    "h_dim = 128 \n",
    "## COMMENT\n",
    "lr = 1e-3\n",
    "\n",
    "## Helper functions.\n",
    "def mnist_mb(mb_size):\n",
    "    \"\"\"Sample batch of size mb_size from training data\"\"\"\n",
    "    yield mnist_images[np.random.choice(N_samples, size=mb_size, replace=True),]\n",
    "def init_weight(size):\n",
    "    return Variable(torch.randn(*size) * (1. / np.sqrt(size[0] / 2.)), requires_grad=True)\n",
    "\n",
    "## COMMENT\n",
    "Wxh = init_weight(size=[X_dim, h_dim])\n",
    "bxh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "Whz_mu = init_weight(size=[h_dim, Z_dim])\n",
    "bhz_mu = Variable(torch.zeros(Z_dim), requires_grad=True)\n",
    "Whz_var = init_weight(size=[h_dim, Z_dim])\n",
    "bhz_var = Variable(torch.zeros(Z_dim), requires_grad=True)\n",
    "\n",
    "## COMMENT\n",
    "def Q(X):\n",
    "    h = nn.relu(X.mm(Wxh) + bxh.repeat(X.size(0), 1))\n",
    "    z_mu = h.mm(Whz_mu) + bhz_mu.repeat(h.size(0), 1)\n",
    "    z_var = h.mm(Whz_var) + bhz_var.repeat(h.size(0), 1)\n",
    "    return z_mu, z_var\n",
    "\n",
    "## COMMENT\n",
    "def sample_z(mu, log_var):\n",
    "    eps = Variable(torch.randn(mb_size, Z_dim))\n",
    "    return mu + torch.exp(log_var / 2) * eps\n",
    "\n",
    "## COMMENT\n",
    "Wzh = init_weight(size=[Z_dim, h_dim])\n",
    "bzh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "Whx = init_weight(size=[h_dim, X_dim])\n",
    "bhx = Variable(torch.zeros(X_dim), requires_grad=True)\n",
    "\n",
    "## COMMENT\n",
    "def P(z):\n",
    "    h = nn.relu(z.mm(Wzh) + bzh.repeat(z.size(0), 1))\n",
    "    X = nn.sigmoid(h.mm(Whx) + bhx.repeat(h.size(0), 1))\n",
    "    return X\n",
    "\n",
    "## COMMENT\n",
    "params = [Wxh, bxh, Whz_mu, bhz_mu, Whz_var, bhz_var, Wzh, bzh, Whx, bhx]\n",
    "solver = torch.optim.Adagrad(params, lr=lr)\n",
    "\n",
    "def run(num_iter):\n",
    "    for iter in range(num_iter):\n",
    "        ## Load data.\n",
    "        X = mnist_mb(mb_size=mb_size).next()\n",
    "        X = Variable(torch.from_numpy(X))\n",
    "\n",
    "        ## COMMENT\n",
    "        z_mu, z_var = Q(X)\n",
    "        z = sample_z(z_mu, z_var)\n",
    "        X_sample = P(z)\n",
    "\n",
    "        ## COMMENT\n",
    "        recon_loss = nn.binary_cross_entropy(X_sample, X, size_average=False) / mb_size\n",
    "        ## COMMENT\n",
    "        kl_loss = torch.mean(0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1. - z_var, 1))\n",
    "        loss = recon_loss + kl_loss\n",
    "\n",
    "        ## COMMENT\n",
    "        loss.backward()\n",
    "\n",
    "        solver.step()\n",
    "        for p in params:\n",
    "            p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.3 Visualize latent space (15)\n",
    "Implement the auto-encoder now with a 2-dimensional latent space, and train again over the MNIST data. Make a visualization of the learned manifold by using a linearly spaced coordinate grid as input for the latent space. \n",
    "\n",
    "Compare your result to Kingma 2014, Appendix A. What are the differences and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
