{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Independent Component Analysis\n",
    "\n",
    "### Machine Learning 2 (2016/2017)\n",
    "\n",
    "* The lab exercises should be made in groups of two people.\n",
    "* The deadline is Wednesday, April 19, 23:59.\n",
    "* Assignment should be sent to p.j.j.p.versteeg@uva.nl. The subject line of your email should be \"[ML2_2017] lab#_lastname1\\_lastname2\". \n",
    "* Put your and your teammates' names in the body of the email\n",
    "* Attach the .IPYNB (IPython Notebook) file containing your code and answers. Naming of the file follows the same rule as the subject line. For example, if the subject line is \"[ML2_2016] lab01\\_Bongers\\_Blom\", the attached file should be \"lab01\\_Bongers\\_Blom.ipynb\". Only use underscores (\"\\_\") to connect names, otherwise the files cannot be parsed.\n",
    "\n",
    "Notes on implementation:\n",
    "\n",
    "* You should write your code and answers in an IPython Notebook: http://ipython.org/notebook.html. If you have problems, please ask.\n",
    "* Use __one cell__ for code and markdown answers only!\n",
    "    * Put all code in the cell with the `# YOUR CODE HERE' comment.\n",
    "    * For theoretical question, put your solution in the YOUR ANSWER HERE cell.\n",
    "* Among the first lines of your notebook should be \"%pylab inline\". This imports all required modules, and your plots will appear inline.\n",
    "* NOTE: test your code and make sure we can run your notebook / scripts!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Literature\n",
    "In this assignment, we will implement the Independent Component Analysis algorithm as described in chapter 34 of David MacKay's book \"Information Theory, Inference, and Learning Algorithms\", which is freely available here:\n",
    "http://www.inference.phy.cam.ac.uk/mackay/itila/book.html\n",
    "\n",
    "Read the ICA chapter carefuly before you continue!\n",
    "\n",
    "### Notation\n",
    "\n",
    "$\\mathbf{X}$ is the $M \\times T$ data matrix, containing $M$ measurements at $T$ time steps.\n",
    "\n",
    "$\\mathbf{S}$ is the $S \\times T$ source matrix, containing $S$ source signal values at $T$ time steps. We will assume $S = M$.\n",
    "\n",
    "$\\mathbf{A}$ is the mixing matrix. We have $\\mathbf{X} = \\mathbf{A S}$.\n",
    "\n",
    "$\\mathbf{W}$ is the matrix we aim to learn. It is the inverse of $\\mathbf{A}$, up to indeterminacies (scaling and permutation of sources).\n",
    "\n",
    "$\\phi$ is an elementwise non-linearity or activation function, typically applied to elements of $\\mathbf{W X}$.\n",
    "\n",
    "### Code\n",
    "In the following assignments, you can make use of the signal generators listed below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "# Signal generators\n",
    "def sawtooth(x, period=0.2, amp=1.0, phase=0.):\n",
    "    return (((x / period - phase - 0.5) % 1) - 0.5) * 2 * amp\n",
    "\n",
    "def sine_wave(x, period=0.2, amp=1.0, phase=0.):\n",
    "    return np.sin((x / period - phase) * 2 * np.pi) * amp\n",
    "\n",
    "def square_wave(x, period=0.2, amp=1.0, phase=0.):\n",
    "    return ((np.floor(2 * x / period - 2 * phase - 1) % 2 == 0).astype(float) - 0.5) * 2 * amp\n",
    "\n",
    "def triangle_wave(x, period=0.2, amp=1.0, phase=0.):\n",
    "    return (sawtooth(x, period, 1., phase) * square_wave(x, period, 1., phase) + 0.5) * 2 * amp\n",
    "\n",
    "def random_nonsingular_matrix(d=2):\n",
    "    \"\"\"\n",
    "    Generates a random nonsingular (invertible) matrix of shape d*d\n",
    "    \"\"\"\n",
    "    epsilon = 0.1\n",
    "    A = np.random.rand(d, d)\n",
    "    while abs(np.linalg.det(A)) < epsilon:\n",
    "        A = np.random.rand(d, d)\n",
    "    return A\n",
    "\n",
    "def plot_signals(X):\n",
    "    \"\"\"\n",
    "    Plot the signals contained in the rows of X.\n",
    "    \"\"\"\n",
    "    figure()\n",
    "    for i in range(X.shape[0]):\n",
    "        ax = plt.subplot(X.shape[0], 1, i + 1)\n",
    "        plot(X[i, :])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code generates some toy data to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate data\n",
    "num_sources = 5\n",
    "signal_length = 500\n",
    "t = linspace(0, 1, signal_length)\n",
    "S = np.c_[sawtooth(t), sine_wave(t, 0.3), square_wave(t, 0.4), triangle_wave(t, 0.25), np.random.randn(t.size)].T\n",
    "plot_signals(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Make mixtures (5 points)\n",
    "Write a function `make_mixtures(S, A)' that takes a matrix of source signals $\\mathbf{S}$ and a mixing matrix $\\mathbf{A}$, and generates mixed signals $\\mathbf{X}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "c682189366f58735ee6e4cf66c42fd43",
     "grade": false,
     "grade_id": "q1",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### 1.1 Make mixtures\n",
    "def make_mixtures(S, A=None):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "A = random_nonsingular_matrix(d=S.shape[0])\n",
    "X = make_mixtures(S, A)\n",
    "plot_signals(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Histogram (5 points)\n",
    "Write a function `plot_histograms(X)` that takes a data-matrix $\\mathbf{X}$ and plots one histogram for each signal (row) in $\\mathbf{X}$. You can use the numpy `histogram()` function. \n",
    "\n",
    "Plot histograms of the sources and the measurements. \n",
    "Which of these distributions (sources or measurements) tend to look more like Gaussians? Can you think of an explanation for this phenomenon? Why is this important for ICA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "a8d44876a72298fe5657facdaea4aa98",
     "grade": false,
     "grade_id": "q2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### 1.2 Histogram\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "68363e5a9239df04a2edb134f6fef78f",
     "grade": true,
     "grade_id": "q2_md",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Implicit priors (20 points)\n",
    "As explained in MacKay's book, an activation function $\\phi$ used in the ICA learning algorithm corresponds to a prior distribution over sources. Specifically, $\\phi(a) = \\frac{d}{da} \\ln p(a)$. For each of the following activation functions, derive the source distribution they correspond to.\n",
    "$$\\phi_0(a) = -\\tanh(a)$$\n",
    "$$\\phi_1(a) = -a + \\tanh(a)$$\n",
    "$$\\phi_2(a) = -a^3$$\n",
    "$$\\phi_3(a) = -\\frac{6a}{a^2 + 5}$$\n",
    "\n",
    "The normalizing constant is not required, so an answer of the form $p(a) \\propto \\verb+[answer]+$ is ok.\n",
    "\n",
    "Plot the activation functions and the corresponding prior distributions. Compare the shape of the priors to the histogram you plotted in the last question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c2eb49c475cbf8fec83f7b2d28fd96a2",
     "grade": true,
     "grade_id": "q3_md",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "00433bd37daec39cba958ea2ca198011",
     "grade": false,
     "grade_id": "q3",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### 1.3 Implicit priors (continued)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Whitening (15 points)\n",
    "Some ICA algorithms can only learn from whitened data. Write a method `whiten(X)` that takes a $M \\times T$ data matrix $\\mathbf{X}$ (where $M$ is the dimensionality and $T$ the number of examples) and returns a whitened matrix. If you forgot what whitening is or how to compute it, various good sources are available online, such as http://courses.media.mit.edu/2010fall/mas622j/whiten.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "23c63abcd3e09d85502c0e4ff02541cb",
     "grade": false,
     "grade_id": "q4",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### 1.4 Whitening\n",
    "def whiten(X):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "Xw = whiten(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Interpret results of whitening (10 points)\n",
    "Make scatter plots of the sources, measurements and whitened measurements. Each axis represents a source/measurement and each time-instance is plotted as a dot in this space. You can use the `np.scatter()` function. Describe what you see.\n",
    "\n",
    "Now compute the covariance matrix of the sources, the measurements and the whitened measurements. You can visualize a covariance matrix using this code:\n",
    "```python\n",
    "# Dummy covariance matrix C;\n",
    "C = np.eye(5)  \n",
    "ax = imshow(C, cmap='gray', interpolation='nearest')\n",
    "```\n",
    "\n",
    "Are the signals independent after whitening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "aa8601be6336bdca80c1a10c1a4f1817",
     "grade": true,
     "grade_id": "q5",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### 1.5 Interpret results of whitening\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a0aba315f0a1193482f9ee326b591ac1",
     "grade": true,
     "grade_id": "q5_md",
     "locked": false,
     "points": 4,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Covariance (5 points)\n",
    "Explain what a covariant algorithm is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4212bce8f348cb27b4fc84801b1964e3",
     "grade": true,
     "grade_id": "q6",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Independent Component Analysis (25 points)\n",
    "Implement the covariant ICA algorithm as described in MacKay. Write a function `ICA(X, activation_function, learning_rate)`, that returns the demixing matrix $\\mathbf{W}$. The input `activation_function` should accept a function such as `lambda a: -tanh(a)`. Update the gradient in batch mode, averaging the gradients over the whole dataset for each update. Try to make it efficient, i.e. use matrix operations instead of loops where possible (loops are slow in interpreted languages such as python and matlab, whereas matrix operations are internally computed using fast C code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "c8247258228780b623f38930c1b0fad8",
     "grade": false,
     "grade_id": "q7",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### 1.7 Independent Component Analysis\n",
    "def ICA(X, activation_function, learning_rate):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Experiments  (5 points)\n",
    "Run ICA on the provided signals using each activation function $\\phi_0, \\ldots, \\phi_3$. Plot the retreived signals for each choice of activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "1acca3217bfc2e0830a541d6c1f0bd6c",
     "grade": true,
     "grade_id": "q8",
     "locked": false,
     "points": 6,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# 1.8 Experiments\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Audio demixing (10 points)\n",
    "The 'cocktail party effect' refers to the ability humans have to attend to one speaker in a noisy room. We will now use ICA to solve a similar but somewhat idealized version of this problem. The code below loads 5 sound files and plots them.\n",
    "\n",
    "Use a random non-singular mixing matrix to mix the 5 sound files and save them to disk using the code below so you can listen to them. Plot histograms of the mixed audio and use your ICA implementation to de-mix these and reproduce the original source signals. As in the previous exercise, try each of the activation functions and report your results.\n",
    "\n",
    "Keep in mind that this problem is easier than the real cocktail party problem, because in real life there are often more sources than measurements (we have only two ears!), and the number of sources is unknown and variable. Also, mixing is not instantaneous in real life, because the sound from one source arrives at each ear at a different point in time. If you have time left, you can think of ways to deal with these issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.io.wavfile\n",
    "# Save mixtures to disk, so you can listen to them in your audio player\n",
    "def save_wav(data, out_file, rate):\n",
    "    scaled = np.int16(data / np.max(np.abs(data)) * 32767)\n",
    "    scipy.io.wavfile.write(out_file, rate, scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load audio sources\n",
    "source_files = ['beet.wav', 'beet9.wav', 'beet92.wav', 'mike.wav', 'street.wav']\n",
    "wav_data = []\n",
    "sample_rate = None\n",
    "for f in source_files:\n",
    "    sr, data = scipy.io.wavfile.read(f, mmap=False)\n",
    "    if sample_rate is None:\n",
    "        sample_rate = sr\n",
    "    else:\n",
    "        assert(sample_rate == sr)\n",
    "    wav_data.append(data[:190000])  # cut off the last part so that all signals have same length\n",
    "\n",
    "# Create source and measurement data\n",
    "S_audio = np.c_[wav_data]\n",
    "plot_signals(S_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "574e773dfe8d4a2ce91d667783acf8ba",
     "grade": true,
     "grade_id": "q9",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### 1.9 Audio demixing\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10 Excess Kurtosis (15 points)\n",
    "The (excess) kurtosis is a measure of 'peakedness' of a distribution. It is defined as\n",
    "$$\n",
    "\\verb+Kurt+[X] = \\frac{\\mu_4}{\\sigma^4} - 3 = \\frac{\\operatorname{E}[(X-{\\mu})^4]}{(\\operatorname{E}[(X-{\\mu})^2])^2} - 3\n",
    "$$\n",
    "Here, $\\mu_4$ is known as the fourth moment about the mean, and $\\sigma$ is the standard deviation.\n",
    "The '-3' term is introduced so that a Gaussian random variable has 0 excess kurtosis.\n",
    "We will now try to understand the performance of the various activation functions by considering the kurtosis of the corresponding priors, and comparing those to the empirical kurtosis of our data.\n",
    "\n",
    "#### 1.10.1 (10 points)\n",
    "First, compute analytically the kurtosis of the four priors that you derived from the activation functions before. To do this, you will need the normalizing constant of the distribution, which you can either obtain analytically (good practice!), using computer algebra software (e.g. Sage) or by numerical integration (see scipy.integrate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "91e16cb105f3909ab0fe3c76e8522a24",
     "grade": true,
     "grade_id": "q10_1",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.10.2 (5 points)\n",
    "Now use the `scipy.stats.kurtosis` function, with the `fisher` option set to `True`, to compute the empirical kurtosis of the dummy signals and the real audio signals.\n",
    "\n",
    "Can you use this data to explain the performance of the various activation functions on the synthetic and real data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "abb69b34260231dc721c05556aae55f4",
     "grade": false,
     "grade_id": "q10_2_code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### 1.10.2 Excess Kurtosis\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "38d2dc1d593a547e0fab40695e595e68",
     "grade": true,
     "grade_id": "q10_2_md",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
